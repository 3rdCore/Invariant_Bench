# Invariant Feature Learning Benchmark

This project provides a benchmark for evaluating invariant feature learning algorithms. It is designed to test how different learners perform under varying degrees of spurious correlation and across different dataset sizes. The entire experimental workflow, from training to analysis, is managed through Jupyter Notebooks.

The core of this benchmark is to generate learning curves that show how model performance (e.g., test accuracy, worst-group accuracy) evolves as the amount of training data increases, under different environmental conditions (i.e., levels of spurious correlation).

## Workflow

The project is structured around two main notebooks:

1.  **`subpopbench/pcl.ipynb`**: This is the main notebook for running experiments. It trains specified invariant learning algorithms (e.g., ERM, GroupDRO) on a dataset (e.g., ColoredMNIST) where spurious correlations can be precisely controlled. The notebook iterates through different training set sizes and saves the resulting performance metrics to CSV files.

2.  **`subpopbench/results/analysis/analysis.ipynb`**: This notebook is used for analyzing the results generated by `pcl.ipynb`. It automatically finds and merges all result CSVs, and then generates a series of plots to visualize the learning curves, compare algorithm performance, and analyze the impact of spurious correlations.

## Getting Started

### Installation

First, ensure you have `uv` installed for environment management.

```bash
pip install uv
```

Then, clone the repository and set up the Python environment:

```bash
git clone https://github.com/3rdCore/Invariant_Bench.git
cd Invariant_Bench/SubpopBench/
uv venv .venv --python 3.12
source .venv/bin/activate 
uv pip install -r requirements.txt
```

### Downloading Data

The necessary datasets can be downloaded and prepared by running:

```bash
python -m subpopbench.scripts.download --data_path <path_to_your_data_folder> --download
```
*Note: For certain medical datasets, manual download might be required. Please refer to the original documentation if needed.*

## Running Experiments

Experiments are launched using shell scripts that execute the `pcl.ipynb` notebook with different parameters.

### Running a Single Experiment

To run a single experiment with default parameters, you can use the `run_single_notebook.sh` script. This is useful for testing or a quick run.

```bash
sbatch run_single_notebook.sh
```

This script will submit a job to SLURM, and the results, including the executed notebook and log files, will be saved in a timestamped folder inside `subpopbench/results/`.

### Running a Sweep of Experiments

For a comprehensive benchmark, you can run a sweep over multiple seeds and spurious correlation settings using `run_multiple_notebook.sh`.

```bash
./run_multiple_notebook.sh
```

This script will:
1.  Define a set of spurious correlation probabilities to test.
2.  Loop through each correlation setting and a range of seeds (0 to 4 by default).
3.  For each combination, it generates and submits a separate SLURM job.
4.  Environment variables (`CMNIST_SPUR_PROB`, `SEED`, etc.) are passed to the notebook to configure each run.
5.  Results for all runs are organized into a single timestamped directory under `subpopbench/results/`, with subdirectories for each parameter combination and seed.

## Analyzing Results

After your experiment runs are complete, you can analyze the results using the `subpopbench/results/analysis/analysis.ipynb` notebook. 

1.  Open the notebook in a Jupyter environment.
2.  Make sure the `local_data_path` variable in the notebook points to the directory containing your CSV results (e.g., `subpopbench/data`).
3.  Run the cells to merge all CSV files and generate plots for validation loss, test accuracy, and worst-group accuracy across different training sizes and spurious correlation settings.

## Hyperparameter Management

The `subpopbench/hparams_registry.py` file manages the hyperparameter configurations for the different algorithms and datasets used in this benchmark. It provides a centralized place to define and access default parameters.

For most use cases, **it is not necessary to modify this file directly**. The experiments are designed to be controlled from the `pcl.ipynb` notebook, where you can easily override default hyperparameters as needed for your specific runs. This approach keeps the core configurations stable while allowing for flexibility in experimentation.

## Acknowledgements
This code is partly based on the open-source implementations from [SubpopBench](https://github.com/YyzHarry/SubpopBench), [DomainBed](https://github.com/facebookresearch/DomainBed), [spurious_feature_learning](https://github.com/izmailovpavel/spurious_feature_learning), and [multi-domain-imbalance](https://github.com/YyzHarry/multi-domain-imbalance).
